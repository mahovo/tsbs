---
title: "DCC Inference Demo"
author: "tsbs package"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 6,
  message = FALSE,
  warning = FALSE
)
devtools::load_all()
```

## Overview

This demo walks through DCC(1,1) parameter inference using the tsbs package.
We'll simulate data, estimate parameters, and compare different inference methods.

**Key takeaway**: Hessian-based standard errors severely underestimate uncertainty
for beta in high-persistence DCC models. Use bootstrap or profile likelihood instead.

```{r load-packages}
library(tsbs)
```

## Step 1: Simulate DCC Data

We generate bivariate data from a true DCC(1,1) process with known parameters.

```{r simulate-data-setup}
## True parameters
alpha_true <- 0.05
beta_true <- 0.90
persistence_true <- alpha_true + beta_true

## Simulation settings
set.seed(42)
n <- 1000  # Number of observations
k <- 2     # Number of series

## Unconditional correlation matrix
Qbar_true <- matrix(c(1.0, 0.5,
                      0.5, 1.0), nrow = 2)

## Initialize storage
z <- matrix(0, n, k)
Q <- array(0, dim = c(k, k, n))
R <- array(0, dim = c(k, k, n))

## Initial values
Q[,,1] <- Qbar_true
R[,,1] <- Qbar_true

## First observation
L1 <- chol(Qbar_true)
z[1, ] <- as.vector(t(L1) %*% rnorm(k))

## Simulate DCC process
for (t in 2:n) {
  ## Update Q using lagged z
  z_lag <- z[t-1, , drop = FALSE]
  Q[,,t] <- (1 - alpha_true - beta_true) * Qbar_true + 
            alpha_true * (t(z_lag) %*% z_lag) + 
            beta_true * Q[,,t-1]
  
  ## Normalize to correlation matrix
  d_t <- sqrt(diag(Q[,,t]))
  D_inv <- diag(1 / d_t, k)
  R[,,t] <- D_inv %*% Q[,,t] %*% D_inv
  
  ## Ensure valid correlation matrix
  R_t <- (R[,,t] + t(R[,,t])) / 2
  diag(R_t) <- 1
  
  ## Generate correlated innovations
  L <- chol(R_t)
  z[t, ] <- as.vector(t(L) %*% rnorm(k))
}

## Prepare estimation inputs
std_resid <- z
weights <- rep(1, n)
Qbar <- cor(std_resid)  # Sample correlation as Qbar estimate
```

```{r simulate-data-output, echo=FALSE}
cat("True parameters:\n")
cat("  alpha =", alpha_true, "\n")
cat("  beta  =", beta_true, "\n")
cat("  persistence =", persistence_true, "\n")
cat("\nSimulation complete:", n, "observations generated\n")
cat("Sample correlation:\n")
print(round(Qbar, 3))
```

## Step 2: Visualize the Dynamic Correlations

```{r plot-correlations, fig.height=4}
## Compute rolling correlation for visualization
window <- 50
rolling_cor <- sapply((window+1):n, function(t) {
  cor(std_resid[(t-window):t, 1], std_resid[(t-window):t, 2])
})

## True time-varying correlation
true_cor <- sapply(1:n, function(t) R[1, 2, t])

plot(1:n, true_cor, type = "l", col = "blue", 
     ylim = c(0, 1), xlab = "Time", ylab = "Correlation",
     main = "Dynamic Correlation")
lines((window+1):n, rolling_cor, col = "red", lty = 2)
abline(h = Qbar_true[1,2], col = "gray", lty = 3)
legend("topright", 
       legend = c("True R_t", paste0("Rolling (", window, ")"), "Unconditional"),
       col = c("blue", "red", "gray"), lty = c(1, 2, 3))
```

## Step 3: Find MLE

```{r find-mle}
## Optimize the DCC negative log-likelihood
mle_result <- optim(
  par = c(0.05, 0.90),
  fn = dcc11_nll,
  method = "L-BFGS-B",
  lower = c(1e-6, 1e-6),
  upper = c(0.5, 0.999),
  std_resid = std_resid,
  weights = weights,
  Qbar = Qbar,
  distribution = "mvn",
  use_reparam = FALSE
)

alpha_mle <- mle_result$par[1]
beta_mle <- mle_result$par[2]
params_mle <- c(alpha = alpha_mle, beta = beta_mle)
```

```{r find-mle-output, echo=FALSE}
cat("MLE estimates:\n")
cat("  alpha =", round(alpha_mle, 4), "(true:", alpha_true, ")\n")
cat("  beta  =", round(beta_mle, 4), "(true:", beta_true, ")\n")
cat("  persistence =", round(alpha_mle + beta_mle, 4), 
    "(true:", persistence_true, ")\n")
cat("\nNegative log-likelihood:", round(mle_result$value, 2), "\n")
cat("Convergence:", ifelse(mle_result$convergence == 0, "Success", "Failed"), "\n")
```

## Step 4: Hessian-based Inference

The Hessian of the likelihood provides standard errors, but these are known
to underestimate uncertainty for beta in high-persistence DCC models.

```{r hessian-inference}
## Compute Hessian and standard errors using dcc11_standard_errors
se_result <- dcc11_standard_errors(
  params = params_mle,
  std_resid = std_resid,
  weights = weights,
  Qbar = Qbar,
  distribution = "mvn",
  method = "hessian"
)

## Store eigenvalue ratio for later use
eig_ratio <- if (!is.null(se_result$eigenvalues) && length(se_result$eigenvalues) >= 2) {
  se_result$eigenvalues[1] / se_result$eigenvalues[2]
} else {
  NA
}
```

```{r hessian-inference-output, echo=FALSE}
cat("Hessian-based inference:\n")
cat("  SE(alpha) =", round(se_result$se["alpha"], 4), "\n")
cat("  SE(beta)  =", round(se_result$se["beta"], 4), "\n")

if (!is.na(eig_ratio)) {
  cat("\nLikelihood surface diagnostics:\n")
  cat("  Eigenvalues:", round(se_result$eigenvalues, 1), "\n")
  cat("  Eigenvalue ratio:", round(eig_ratio, 1), "\n")
  if (!is.null(se_result$condition_number)) {
    cat("  Condition number:", round(se_result$condition_number, 1), "\n")
  }
  
  if (eig_ratio > 10) {
    cat("\n  WARNING: High eigenvalue ratio indicates anisotropic curvature.\n")
    cat("           Hessian SE for beta may be severely underestimated.\n")
  }
} else {
  cat("\nNote: Eigenvalue diagnostics not available\n")
}
```

### Robust Standard Errors with Edge Case Handling

The `compute_dcc_standard_errors_robust()` function provides a higher-level
interface that handles edge cases like boundary estimates and constant correlation.

```{r robust-se}
## Create a result structure like estimate_dcc_parameters_weighted() returns
dcc_result <- list(
  dcc_pars = list(alpha_1 = alpha_mle, beta_1 = beta_mle),
  dist_pars = list(),
  correlation_type = "dynamic"
)

## Compute robust SEs
robust_se <- compute_dcc_standard_errors_robust(
  dcc_result = dcc_result,
  std_resid = std_resid,
  weights = weights,
  Qbar = Qbar,
  distribution = "mvn",
  method = "hessian"
)
```

```{r robust-se-output, echo=FALSE}
cat("Robust SE computation:\n")
cat("  Valid:", robust_se$valid, "\n")
cat("  Reason:", robust_se$reason, "\n")
cat("  Correlation type:", robust_se$correlation_type, "\n")
cat("  SE(alpha) =", round(robust_se$se["alpha"], 4), "\n")
cat("  SE(beta)  =", round(robust_se$se["beta"], 4), "\n")

if (!is.null(robust_se$info_eigenvalues)) {
  cat("  Info eigenvalues:", round(robust_se$info_eigenvalues, 1), "\n")
}
```

### Full Estimation Summary

The `dcc11_estimation_summary()` function provides a comprehensive summary
including parameter estimates, standard errors, confidence intervals, and
diagnostic information.

```{r estimation-summary}
## Get full estimation summary
summary_result <- dcc11_estimation_summary(
  params = params_mle,
  std_resid = std_resid,
  weights = weights,
  Qbar = Qbar,
  distribution = "mvn",
  use_reparam = FALSE,
  level = 0.95,
  method = "hessian"
)

## Print the summary (uses the print.dcc11_summary method)
print(summary_result)
```

### Effect of Reparameterization on Standard Errors

The DCC model can be estimated in the original (α, β) space or in an
unconstrained (ψ, φ) space. Let's compare the standard errors.

```{r reparam-comparison}
## Convert MLE to reparameterized space
reparam_mle <- dcc11_to_unconstrained(alpha_mle, beta_mle)
params_reparam <- c(psi = reparam_mle["psi"], phi = reparam_mle["phi"])

## Compute SEs in reparameterized space
se_reparam <- dcc11_standard_errors(
  params = params_reparam,
  std_resid = std_resid,
  weights = weights,
  Qbar = Qbar,
  distribution = "mvn",
  method = "hessian",
  use_reparam = TRUE
)

## Transform SEs back to original space using delta method
se_transformed <- dcc11_transform_se(se_reparam, to_reparam = FALSE)

## Eigenvalue ratio in reparameterized space
eig_ratio_reparam <- if (!is.null(se_reparam$eigenvalues) && length(se_reparam$eigenvalues) >= 2) {
  se_reparam$eigenvalues[1] / se_reparam$eigenvalues[2]
} else {
  NA
}
```

```{r reparam-comparison-output, echo=FALSE}
cat("Reparameterized MLE:\n")
cat("  psi =", round(params_reparam["psi"], 4), "\n")
cat("  phi =", round(params_reparam["phi"], 4), "\n")

cat("\nStandard errors in (psi, phi) space:\n")
cat("  SE(psi) =", round(se_reparam$se["psi"], 4), "\n")
cat("  SE(phi) =", round(se_reparam$se["phi"], 4), "\n")

cat("\nTransformed SEs (delta method, back to alpha/beta):\n")
cat("  SE(alpha) =", round(se_transformed$se["alpha"], 4), "\n")
cat("  SE(beta)  =", round(se_transformed$se["beta"], 4), "\n")

cat("\nComparison of SE methods:\n")
cat("  Direct (alpha, beta):      SE(alpha) =", round(se_result$se["alpha"], 4),
    ", SE(beta) =", round(se_result$se["beta"], 4), "\n")
cat("  Via reparameterization:    SE(alpha) =", round(se_transformed$se["alpha"], 4),
    ", SE(beta) =", round(se_transformed$se["beta"], 4), "\n")

if (!is.na(eig_ratio) && !is.na(eig_ratio_reparam)) {
  cat("\nEigenvalue ratio comparison:\n")
  cat("  Original space:        ", round(eig_ratio, 1), "\n")
  cat("  Reparameterized space: ", round(eig_ratio_reparam, 1), "\n")
}
```

## Step 5: Likelihood Surface Visualization

Let's visualize why the Hessian-based SE for beta is unreliable.

```{r likelihood-surface, fig.height=5}
## Create grid around MLE
alpha_grid <- seq(max(0.001, alpha_mle - 0.05), 
                  min(0.3, alpha_mle + 0.05), 
                  length.out = 40)
beta_grid <- seq(max(0.5, beta_mle - 0.15), 
                 min(0.999, beta_mle + 0.05), 
                 length.out = 40)

## Compute NLL on grid
nll_surface <- matrix(NA, length(alpha_grid), length(beta_grid))
for (i in seq_along(alpha_grid)) {
  for (j in seq_along(beta_grid)) {
    if (alpha_grid[i] + beta_grid[j] < 0.9999) {
      nll_surface[i, j] <- dcc11_nll(
        params = c(alpha_grid[i], beta_grid[j]),
        std_resid = std_resid,
        weights = weights,
        Qbar = Qbar,
        distribution = "mvn",
        use_reparam = FALSE
      )
    }
  }
}

## Plot contours
nll_min <- min(nll_surface, na.rm = TRUE)
levels <- nll_min + c(0.5, 1, 2, 5, 10, 20, 50)

contour(alpha_grid, beta_grid, nll_surface,
        levels = levels,
        xlab = expression(alpha), ylab = expression(beta),
        main = "DCC Negative Log-Likelihood Surface")
points(alpha_mle, beta_mle, pch = 19, col = "red", cex = 1.5)
points(alpha_true, beta_true, pch = 4, col = "blue", cex = 1.5, lwd = 2)
legend("topright", legend = c("MLE", "True"), 
       pch = c(19, 4), col = c("red", "blue"))
```

Notice how the contours are elongated in the beta direction - this is the
"flat beta problem" that causes Hessian-based SEs to underestimate uncertainty.

## Step 6: Profile Likelihood

Profile likelihood provides confidence intervals that don't assume a quadratic
likelihood surface. We use `dcc11_profile_ci` for this.

```{r profile-likelihood}
## Compute profile CI for beta
profile_ci_beta <- dcc11_profile_ci(
  params = params_mle,
  std_resid = std_resid,
  weights = weights,
  Qbar = Qbar,
  distribution = "mvn",
  use_reparam = FALSE,
  param_idx = 2,  # beta
  level = 0.95,
  n_grid = 50
)

## Compare to Hessian-based CI
hess_ci_width <- 2 * 1.96 * se_result$se["beta"]

## Also compute profile CI for alpha
profile_ci_alpha <- dcc11_profile_ci(
  params = params_mle,
  std_resid = std_resid,
  weights = weights,
  Qbar = Qbar,
  distribution = "mvn",
  use_reparam = FALSE,
  param_idx = 1,  # alpha
  level = 0.95,
  n_grid = 50
)
```

```{r profile-likelihood-output, echo=FALSE}
cat("95% Profile Likelihood CI for beta:\n")
cat("  [", round(profile_ci_beta["lower"], 4), ",", 
    round(profile_ci_beta["upper"], 4), "]\n")
cat("  Width:", round(profile_ci_beta["upper"] - profile_ci_beta["lower"], 4), "\n")

cat("\nHessian-based 95% CI width:", round(hess_ci_width, 4), "\n")
cat("Ratio (Profile/Hessian):", 
    round((profile_ci_beta["upper"] - profile_ci_beta["lower"]) / hess_ci_width, 1), "x\n")

cat("\n95% Profile Likelihood CI for alpha:\n")
cat("  [", round(profile_ci_alpha["lower"], 4), ",", 
    round(profile_ci_alpha["upper"], 4), "]\n")
```

```{r profile-plot, fig.height=4}
## Visualize profile likelihood for beta
beta_profile_grid <- seq(max(0.5, beta_mle - 0.25), 
                         min(0.999, beta_mle + 0.08), 
                         length.out = 50)

profile_nll <- sapply(beta_profile_grid, function(b) {
  if (b >= 0.999) return(NA)
  opt <- optim(
    par = alpha_mle,
    fn = function(a) {
      if (a + b >= 0.9999 || a < 0) return(1e10)
      dcc11_nll(c(a, b), std_resid, weights, Qbar, "mvn", FALSE)
    },
    method = "L-BFGS-B",
    lower = 1e-6,
    upper = min(0.5, 0.9999 - b)
  )
  opt$value
})

chi2_95 <- qchisq(0.95, df = 1) / 2

plot(beta_profile_grid, profile_nll - nll_min, type = "l", lwd = 2,
     xlab = expression(beta), ylab = "Profile NLL - min(NLL)",
     main = "Profile Likelihood for Beta")
abline(h = chi2_95, col = "red", lty = 2)
abline(v = beta_mle, col = "blue", lty = 3)
abline(v = beta_true, col = "darkgreen", lty = 3)
legend("topright", 
       legend = c("Profile NLL", "95% threshold", "MLE", "True"),
       col = c("black", "red", "blue", "darkgreen"), 
       lty = c(1, 2, 3, 3), lwd = c(2, 1, 1, 1))
```

## Step 7: Bootstrap Inference

Bootstrap provides reliable standard errors by resampling the data.

```{r bootstrap-setup}
## Residual bootstrap setup
set.seed(123)
n_boot <- 200

boot_estimates <- matrix(NA, n_boot, 2)
colnames(boot_estimates) <- c("alpha", "beta")
```

```{r bootstrap-setup-output, echo=FALSE}
cat("Running", n_boot, "bootstrap replications...\n")
```

```{r bootstrap-loop, results='hide'}
pb <- txtProgressBar(min = 0, max = n_boot, style = 3)

for (b in 1:n_boot) {
  ## Resample rows (residual bootstrap)
  idx <- sample(1:n, n, replace = TRUE)
  boot_resid <- std_resid[idx, ]
  boot_Qbar <- cor(boot_resid)
  
  ## Re-estimate
  boot_opt <- tryCatch({
    optim(
      par = params_mle,  # Start from MLE
      fn = dcc11_nll,
      method = "L-BFGS-B",
      lower = c(1e-6, 1e-6),
      upper = c(0.5, 0.999),
      std_resid = boot_resid,
      weights = rep(1, n),
      Qbar = boot_Qbar,
      distribution = "mvn",
      use_reparam = FALSE
    )
  }, error = function(e) NULL)
  
  if (!is.null(boot_opt) && boot_opt$convergence == 0) {
    boot_estimates[b, ] <- boot_opt$par
  }
  
  setTxtProgressBar(pb, b)
}

close(pb)
```

```{r bootstrap-results}
## Compute bootstrap statistics
valid_boots <- complete.cases(boot_estimates)

boot_se <- apply(boot_estimates[valid_boots, ], 2, sd)
boot_ci <- apply(boot_estimates[valid_boots, ], 2, quantile, 
                 probs = c(0.025, 0.975))
```

```{r bootstrap-results-output, echo=FALSE}
cat("Valid bootstrap replications:", sum(valid_boots), "/", n_boot, "\n")

cat("\nBootstrap standard errors:\n")
cat("  SE(alpha) =", round(boot_se["alpha"], 4), "\n")
cat("  SE(beta)  =", round(boot_se["beta"], 4), "\n")

cat("\nBootstrap 95% percentile CIs:\n")
cat("  alpha: [", round(boot_ci[1, "alpha"], 4), ",", 
    round(boot_ci[2, "alpha"], 4), "]\n")
cat("  beta:  [", round(boot_ci[1, "beta"], 4), ",", 
    round(boot_ci[2, "beta"], 4), "]\n")
```

```{r bootstrap-plot, fig.height=4}
## Visualize bootstrap distribution
par(mfrow = c(1, 2))

hist(boot_estimates[valid_boots, "alpha"], breaks = 30, 
     main = expression("Bootstrap distribution of " * alpha),
     xlab = expression(alpha), col = "lightblue")
abline(v = alpha_mle, col = "red", lwd = 2)
abline(v = alpha_true, col = "blue", lwd = 2, lty = 2)

hist(boot_estimates[valid_boots, "beta"], breaks = 30,
     main = expression("Bootstrap distribution of " * beta),
     xlab = expression(beta), col = "lightblue")
abline(v = beta_mle, col = "red", lwd = 2)
abline(v = beta_true, col = "blue", lwd = 2, lty = 2)

par(mfrow = c(1, 1))
```

## Step 8: Comparison of Inference Methods

```{r comparison, echo=FALSE}
## Summary table
comparison <- data.frame(
  Parameter = c("alpha", "beta"),
  MLE = round(params_mle, 4),
  True = c(alpha_true, beta_true),
  Hessian_SE = round(se_result$se, 4),
  Bootstrap_SE = round(boot_se, 4),
  SE_Ratio = round(se_result$se / boot_se, 2)
)

cat("\n========================================\n")
cat("       INFERENCE METHOD COMPARISON\n")
cat("========================================\n\n")
print(comparison, row.names = FALSE)

cat("\n\nKey findings:\n")
cat("- Hessian SE / Bootstrap SE ratio for alpha:", comparison$SE_Ratio[1], "\n")
cat("- Hessian SE / Bootstrap SE ratio for beta:", comparison$SE_Ratio[2], "\n")

if (comparison$SE_Ratio[2] < 0.5) {
  cat("\n*** WARNING: Hessian SE for beta is less than half of bootstrap SE ***\n")
  cat("    This confirms the 'flat beta problem' - use bootstrap for inference.\n")
}
```

```{r summary, echo=FALSE}
cat("\n")
cat("=============================================\n")
cat("         RECOMMENDED INFERENCE RESULTS\n")
cat("=============================================\n\n")

cat("Parameter estimates:\n")
cat("  alpha =", round(alpha_mle, 4), 
    "(Hessian SE:", round(se_result$se["alpha"], 4), ")\n")
cat("  beta  =", round(beta_mle, 4), 
    "(Bootstrap 95% CI: [", round(boot_ci[1, "beta"], 3), ",",
    round(boot_ci[2, "beta"], 3), "])\n")
cat("  persistence =", round(alpha_mle + beta_mle, 4), "\n")

cat("\nConfidence intervals comparison:\n")
cat("  Beta - Hessian 95% CI:  [", 
    round(beta_mle - 1.96 * se_result$se["beta"], 4), ",",
    round(beta_mle + 1.96 * se_result$se["beta"], 4), "] (width:",
    round(2 * 1.96 * se_result$se["beta"], 4), ")\n")
cat("  Beta - Profile 95% CI:  [", 
    round(profile_ci_beta["lower"], 4), ",",
    round(profile_ci_beta["upper"], 4), "] (width:",
    round(profile_ci_beta["upper"] - profile_ci_beta["lower"], 4), ")\n")
cat("  Beta - Bootstrap 95% CI:[", 
    round(boot_ci[1, "beta"], 4), ",",
    round(boot_ci[2, "beta"], 4), "] (width:",
    round(boot_ci[2, "beta"] - boot_ci[1, "beta"], 4), ")\n")

cat("\nRecommendations:\n")
cat("1. For alpha: Hessian SE is acceptable (ratio ~", 
    round(comparison$SE_Ratio[1], 1), ")\n")
cat("2. For beta: Use bootstrap or profile CI (Hessian underestimates by ~",
    round(1/comparison$SE_Ratio[2], 1), "x)\n")
if (!is.na(eig_ratio)) {
  cat("3. The eigenvalue ratio of", round(eig_ratio, 1), 
      "confirms anisotropic curvature\n")
}
```

## Session Info

```{r session-info}
sessionInfo()
```
